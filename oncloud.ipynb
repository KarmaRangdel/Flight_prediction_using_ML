{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16d4db3-921f-48d9-8926-1247196496cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step1\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Setup - minimal imports\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = \"c182567a4701745l12354044t1w717192035388-labbucket-qyu2bpf3cfmc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b458d1-cd06-4937-9b34-ba6e3e4179f1",
   "metadata": {
    "id": "d1b458d1-cd06-4937-9b34-ba6e3e4179f1",
    "outputId": "94e05075-3cce-4da7-c810-87077767ab3f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING COMPREHENSIVE MODEL COMPARISON\n",
      "\n",
      "============================================================\n",
      "PROCESSING V1: combined_cvs_v1_small.csv\n",
      "============================================================\n",
      "\n",
      "=== Training linear on combined_cvs_v1_small.csv ===\n",
      "Data split: 88800 train, 22199 test\n",
      "Data uploaded to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: linear-learner-2025-10-31-14-17-31-995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "2025-10-31 14:17:35 Starting - Starting the training job...\n",
      "2025-10-31 14:17:51 Starting - Preparing the instances for training...\n",
      "2025-10-31 14:18:14 Downloading - Downloading input data...\n",
      "2025-10-31 14:19:00 Downloading - Downloading the training image.........\n",
      "2025-10-31 14:20:26 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 14:21:01 Uploading - Uploading generated training model...\n",
      "2025-10-31 14:21:19 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linear-learner-2025-10-31-14-21-49-299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 185\n",
      "Billable seconds: 185\n",
      "Training completed!\n",
      "Running batch transform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: linear-learner-2025-10-31-14-21-49-942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................\n",
      "...Predictions completed!\n",
      "Parsed 22199 predictions\n",
      "LINEAR Accuracy: 0.7589\n",
      "Sample - True: [1 0 1], Pred: [0, 0, 0]\n",
      "\n",
      "=== Training xgboost on combined_cvs_v1_small.csv ===\n",
      "Data split: 88800 train, 22199 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-31-14-27-56-631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to S3\n",
      "Training started...\n",
      "2025-10-31 14:27:57 Starting - Starting the training job...\n",
      "2025-10-31 14:28:12 Starting - Preparing the instances for training...\n",
      "2025-10-31 14:28:36 Downloading - Downloading input data...\n",
      "2025-10-31 14:29:27 Downloading - Downloading the training image......\n",
      "2025-10-31 14:30:28 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 14:31:01 Uploading - Uploading generated training model\n",
      "2025-10-31 14:31:01 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-31-14-31-13-441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 145\n",
      "Billable seconds: 145\n",
      "Training completed!\n",
      "Running batch transform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-31-14-31-14-084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "...Predictions completed!\n",
      "Parsed 22199 predictions\n",
      "XGBOOST Accuracy: 0.7651\n",
      "Sample - True: [1 0 1], Pred: [0, 0, 0]\n",
      "\n",
      "V1 SIMPLE MODELS RESULTS:\n",
      "  Linear Learner: 0.7589\n",
      "  XGBoost: 0.7651\n",
      "\n",
      "============================================================\n",
      "PROCESSING V2: combined_cvs_v2_small.csv\n",
      "============================================================\n",
      "\n",
      "=== Training linear on combined_cvs_v2_small.csv ===\n",
      "Data split: 88080 train, 22019 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: linear-learner-2025-10-31-14-36-50-013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to S3\n",
      "Training started...\n",
      "2025-10-31 14:36:51 Starting - Starting the training job...\n",
      "2025-10-31 14:37:07 Starting - Preparing the instances for training...\n",
      "2025-10-31 14:37:28 Downloading - Downloading input data...\n",
      "2025-10-31 14:38:14 Downloading - Downloading the training image.........\n",
      "2025-10-31 14:39:30 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 14:40:23 Uploading - Uploading generated training model\n",
      "2025-10-31 14:40:23 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: linear-learner-2025-10-31-14-40-37-032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 175\n",
      "Billable seconds: 175\n",
      "Training completed!\n",
      "Running batch transform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: linear-learner-2025-10-31-14-40-37-655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\n",
      "...Predictions completed!\n",
      "Parsed 22019 predictions\n",
      "LINEAR Accuracy: 0.7580\n",
      "Sample - True: [0 1 1], Pred: [0, 0, 0]\n",
      "\n",
      "=== Training xgboost on combined_cvs_v2_small.csv ===\n",
      "Data split: 88080 train, 22019 test\n",
      "Data uploaded to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-31-14-47-44-458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "2025-10-31 14:47:45 Starting - Starting the training job...\n",
      "2025-10-31 14:47:59 Starting - Preparing the instances for training...\n",
      "2025-10-31 14:48:24 Downloading - Downloading input data...\n",
      "2025-10-31 14:49:10 Downloading - Downloading the training image......\n",
      "2025-10-31 14:50:10 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 14:50:44 Uploading - Uploading generated training model\n",
      "2025-10-31 14:50:44 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-31-14-51-01-335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 140\n",
      "Billable seconds: 140\n",
      "Training completed!\n",
      "Running batch transform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-31-14-51-01-905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "...Predictions completed!\n",
      "Parsed 22019 predictions\n",
      "XGBOOST Accuracy: 0.7678\n",
      "Sample - True: [0 1 1], Pred: [0, 0, 0]\n",
      "\n",
      "V2 SIMPLE MODELS RESULTS:\n",
      "  Linear Learner: 0.7580\n",
      "  XGBoost: 0.7678\n",
      "\n",
      "================================================================================\n",
      "STEP 3: BUILDING AND EVALUATING ENSEMBLE MODELS\n",
      "================================================================================\n",
      "\n",
      "BUILDING ENSEMBLE MODEL FOR V1\n",
      "\n",
      "STEP 3: BUILDING ENSEMBLE MODEL FOR V1\n",
      "Dataset: combined_cvs_v1_small.csv\n",
      "Dataset shape: (110999, 94)\n",
      "Target distribution: {0: 83910, 1: 27089}\n",
      "Data split - Train: (77699, 93), Val: (16649, 93), Test: (16651, 93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-31-14-56-38-352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to S3\n",
      "Training ensemble model...\n",
      "2025-10-31 14:56:39 Starting - Starting the training job...\n",
      "2025-10-31 14:56:55 Starting - Preparing the instances for training...\n",
      "2025-10-31 14:57:41 Downloading - Downloading the training image......\n",
      "2025-10-31 14:58:26 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 14:59:02 Uploading - Uploading generated training model...\n",
      "2025-10-31 14:59:15 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-31-14-59-55-769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 120\n",
      "Billable seconds: 120\n",
      "Ensemble training completed!\n",
      "Hosting model on ml.m5.xlarge instance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-31-14-59-56-388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch transform on test data...\n",
      "..............................\n",
      "...Batch transform completed!\n",
      "Parsed 16651 ensemble predictions\n",
      "\n",
      "ENSEMBLE MODEL PERFORMANCE METRICS (V1):\n",
      "   Accuracy:  0.7699\n",
      "   Precision: 0.6790\n",
      "   Recall:    0.1043\n",
      "   F1-Score:  0.1809\n",
      "   Confusion Matrix:\n",
      "     - True Positives:  423\n",
      "     - False Positives: 200\n",
      "     - True Negatives:  12397\n",
      "     - False Negatives: 3631\n",
      "\n",
      "BUILDING ENSEMBLE MODEL FOR V2\n",
      "\n",
      "STEP 3: BUILDING ENSEMBLE MODEL FOR V2\n",
      "Dataset: combined_cvs_v2_small.csv\n",
      "Dataset shape: (110099, 85)\n",
      "Target distribution: {0: 83282, 1: 26817}\n",
      "Data split - Train: (77069, 84), Val: (16514, 84), Test: (16516, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-31-15-05-32-961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to S3\n",
      "Training ensemble model...\n",
      "2025-10-31 15:05:33 Starting - Starting the training job...\n",
      "2025-10-31 15:05:54 Starting - Preparing the instances for training...\n",
      "2025-10-31 15:06:17 Downloading - Downloading input data...\n",
      "2025-10-31 15:06:42 Downloading - Downloading the training image...\n",
      "2025-10-31 15:07:28 Training - Training image download completed. Training in progress....\n",
      "2025-10-31 15:07:58 Uploading - Uploading generated training model...\n",
      "2025-10-31 15:08:12 Completed - Training job completed\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-31-15-08-49-798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 114\n",
      "Billable seconds: 114\n",
      "Ensemble training completed!\n",
      "Hosting model on ml.m5.xlarge instance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-31-15-08-50-510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch transform on test data...\n",
      ".................................\n",
      "...Batch transform completed!\n",
      "Parsed 16516 ensemble predictions\n",
      "\n",
      "ENSEMBLE MODEL PERFORMANCE METRICS (V2):\n",
      "   Accuracy:  0.7698\n",
      "   Precision: 0.6087\n",
      "   Recall:    0.1492\n",
      "   F1-Score:  0.2396\n",
      "   Confusion Matrix:\n",
      "     - True Positives:  599\n",
      "     - False Positives: 385\n",
      "     - True Negatives:  12115\n",
      "     - False Negatives: 3417\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPREHENSIVE COMPARISON: SIMPLE vs ENSEMBLE MODELS\n",
      "================================================================================\n",
      "\n",
      "DATASET V1 COMPARISON:\n",
      "\n",
      "   SIMPLE MODELS:\n",
      "     Linear Learner: 0.7589\n",
      "     XGBoost:       0.7651\n",
      "\n",
      "   ENSEMBLE MODEL (XGBoost Optimized):\n",
      "     Accuracy:      0.7699\n",
      "     Precision:     0.6790\n",
      "     Recall:        0.1043\n",
      "     F1-Score:      0.1809\n",
      "\n",
      " PERFORMANCE COMPARISON:\n",
      "     Best Simple Model:    0.7651\n",
      "     Ensemble Model:       0.7699\n",
      "     Improvement:          +0.0048\n",
      "Ensemble model performed better by 0.0048\n",
      "\n",
      "OBSERVATIONS:\n",
      "     - Ensemble model shows modest improvement\n",
      "     - The dataset might have simpler patterns\n",
      "     - Ensemble benefits are present but limited\n",
      "\n",
      "DATASET V2 COMPARISON:\n",
      "\n",
      "   SIMPLE MODELS:\n",
      "     Linear Learner: 0.7580\n",
      "     XGBoost:       0.7678\n",
      "\n",
      "   ENSEMBLE MODEL (XGBoost Optimized):\n",
      "     Accuracy:      0.7698\n",
      "     Precision:     0.6087\n",
      "     Recall:        0.1492\n",
      "     F1-Score:      0.2396\n",
      "\n",
      " PERFORMANCE COMPARISON:\n",
      "     Best Simple Model:    0.7678\n",
      "     Ensemble Model:       0.7698\n",
      "     Improvement:          +0.0020\n",
      "Ensemble model performed better by 0.0020\n",
      "\n",
      "OBSERVATIONS:\n",
      "     - Ensemble model shows modest improvement\n",
      "     - The dataset might have simpler patterns\n",
      "     - Ensemble benefits are present but limited\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ANALYSIS COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Step 2: build and evaluate simple models\n",
    "\n",
    "def manual_train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_seed=42):\n",
    "\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_train = int(n_samples * train_size)\n",
    "    n_val = int(n_samples * val_size)\n",
    "\n",
    "    # Create indices and shuffle\n",
    "    indices = list(range(n_samples))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:n_train + n_val]\n",
    "    test_indices = indices[n_train + n_val:]\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[train_indices]\n",
    "        X_val = X.iloc[val_indices]\n",
    "        X_test = X.iloc[test_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "        y_val = y.iloc[val_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "    else:\n",
    "        X_train = X[train_indices]\n",
    "        X_val = X[val_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_val = y[val_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_seed=42):\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_test = int(n_samples * test_size)\n",
    "\n",
    "    # Create indices and shuffle\n",
    "    indices = list(range(n_samples))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[train_indices]\n",
    "        X_test = X.iloc[test_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "    else:\n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def manual_accuracy_score(y_true, y_pred):\n",
    "\n",
    "    correct = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None):\n",
    "\n",
    "    # Basic accuracy\n",
    "    accuracy = manual_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Confusion matrix components\n",
    "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
    "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n",
    "    tn = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 0)\n",
    "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n",
    "\n",
    "    # Additional metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': {\n",
    "            'true_positive': tp,\n",
    "            'false_positive': fp,\n",
    "            'true_negative': tn,\n",
    "            'false_negative': fn\n",
    "        }\n",
    "    }\n",
    "\n",
    "def simple_train_test(dataset_name, model_type=\"linear\"):\n",
    "    print(f\"\\n=== Training {model_type} on {dataset_name} ===\")\n",
    "\n",
    "    # 1. Load data\n",
    "    df = pd.read_csv(dataset_name, header=None)\n",
    "\n",
    "    # 2. Convert any boolean columns to 1/0\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    if len(bool_cols) > 0:\n",
    "        df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "    # 3. Split data manually\n",
    "    y = df.iloc[:, 0]  # target\n",
    "    X = df.iloc[:, 1:] # features\n",
    "\n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    print(f\"Data split: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "\n",
    "    # 4. Save as CSV\n",
    "    train_data = pd.concat([y_train, X_train], axis=1)\n",
    "    test_data = X_test  # No labels for prediction\n",
    "\n",
    "    train_file = f\"train_{model_type}.csv\"\n",
    "    test_file = f\"test_{model_type}.csv\"\n",
    "\n",
    "    train_data.to_csv(train_file, header=False, index=False)\n",
    "    test_data.to_csv(test_file, header=False, index=False)\n",
    "\n",
    "    # 5. Upload to S3\n",
    "    prefix = f\"final-{model_type}-{int(time.time())}\"\n",
    "    train_s3 = session.upload_data(train_file, bucket=bucket, key_prefix=f\"{prefix}/train\")\n",
    "    test_s3 = session.upload_data(test_file, bucket=bucket, key_prefix=f\"{prefix}/test\")\n",
    "\n",
    "    print(\"Data uploaded to S3\")\n",
    "\n",
    "    # 6. Get algorithm container\n",
    "    if model_type == \"linear\":\n",
    "        container = image_uris.retrieve(\"linear-learner\", region, \"1\")\n",
    "    else:\n",
    "        container = image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "    # 7. Create and train model\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "        sagemaker_session=session,\n",
    "    )\n",
    "\n",
    "    # Set hyperparameters\n",
    "    if model_type == \"linear\":\n",
    "        estimator.set_hyperparameters(\n",
    "            predictor_type=\"binary_classifier\",\n",
    "            epochs=10,\n",
    "            mini_batch_size=1000\n",
    "        )\n",
    "    else:\n",
    "        estimator.set_hyperparameters(\n",
    "            objective=\"binary:logistic\",\n",
    "            num_round=20\n",
    "        )\n",
    "\n",
    "    # Train\n",
    "    print(\"Training started...\")\n",
    "    train_input = sagemaker.inputs.TrainingInput(train_s3, content_type='text/csv')\n",
    "    estimator.fit({'train': train_input})\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # 8. Use BATCH TRANSFORM (most reliable)\n",
    "    print(\"Running batch transform...\")\n",
    "\n",
    "    transformer = estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        output_path=f's3://{bucket}/{prefix}/predictions'\n",
    "    )\n",
    "\n",
    "    transformer.transform(\n",
    "        data=test_s3,\n",
    "        content_type='text/csv',\n",
    "        split_type='Line'\n",
    "    )\n",
    "\n",
    "    transformer.wait()\n",
    "    print(\"Predictions completed!\")\n",
    "\n",
    "    # 9. Get predictions\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Download prediction results\n",
    "    predictions_key = f\"{prefix}/predictions/test_{model_type}.csv.out\"\n",
    "\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=predictions_key)\n",
    "        predictions_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        # Parse predictions\n",
    "        pred_probs = []\n",
    "        for line in predictions_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                if model_type == \"linear\":\n",
    "                    # Linear Learner returns JSON\n",
    "                    try:\n",
    "                        pred_dict = json.loads(line.strip())\n",
    "                        if 'score' in pred_dict:\n",
    "                            pred_probs.append(pred_dict['score'])\n",
    "                        else:\n",
    "                            # Try to find any numeric value\n",
    "                            for key, value in pred_dict.items():\n",
    "                                if isinstance(value, (int, float)):\n",
    "                                    pred_probs.append(float(value))\n",
    "                                    break\n",
    "                            else:\n",
    "                                pred_probs.append(0.5)\n",
    "                    except:\n",
    "                        # If not JSON, try direct float\n",
    "                        try:\n",
    "                            pred_probs.append(float(line.strip()))\n",
    "                        except:\n",
    "                            pred_probs.append(0.5)\n",
    "                else:\n",
    "                    # XGBoost returns probability directly\n",
    "                    try:\n",
    "                        pred_probs.append(float(line.strip()))\n",
    "                    except:\n",
    "                        pred_probs.append(0.5)\n",
    "\n",
    "        print(f\"Parsed {len(pred_probs)} predictions\")\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in pred_probs]\n",
    "        acc = manual_accuracy_score(y_test.values, pred_labels)\n",
    "\n",
    "        print(f\"{model_type.upper()} Accuracy: {acc:.4f}\")\n",
    "\n",
    "        print(f\"Sample - True: {y_test.values[:3]}, Pred: {pred_labels[:3]}\")\n",
    "        return acc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting predictions: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# Step 3: Build and evaluate ensemble models\n",
    "def build_ensemble_model(dataset_name, dataset_version):\n",
    "    \"\"\"Build and evaluate ensemble model using XGBoost with comprehensive metrics\"\"\"\n",
    "    print(f\"\\nSTEP 3: BUILDING ENSEMBLE MODEL FOR {dataset_version.upper()}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "\n",
    "    # 1. Load and prepare data\n",
    "    df = pd.read_csv(dataset_name, header=None)\n",
    "\n",
    "    # Convert any boolean columns to 1/0\n",
    "    bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "    if len(bool_cols) > 0:\n",
    "        df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "    # Split target and features\n",
    "    y = df.iloc[:, 0]  # first column = target\n",
    "    X = df.iloc[:, 1:] # rest = features\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "    # 2. Split into training, validation, and testing sets (70-15-15)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = manual_train_val_test_split(\n",
    "        X, y, train_size=0.7, val_size=0.15, test_size=0.15\n",
    "    )\n",
    "\n",
    "    print(f\"Data split - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    # 3. Save datasets\n",
    "    train_data = pd.concat([y_train, X_train], axis=1)\n",
    "    val_data = pd.concat([y_val, X_val], axis=1)\n",
    "    test_features = X_test\n",
    "\n",
    "    train_file = f\"train_ensemble_{dataset_version}.csv\"\n",
    "    val_file = f\"val_ensemble_{dataset_version}.csv\"\n",
    "    test_file = f\"test_ensemble_{dataset_version}.csv\"\n",
    "\n",
    "    train_data.to_csv(train_file, header=False, index=False)\n",
    "    val_data.to_csv(val_file, header=False, index=False)\n",
    "    test_features.to_csv(test_file, header=False, index=False)\n",
    "\n",
    "    # 4. Upload to S3\n",
    "    timestamp = str(int(time.time()))\n",
    "    prefix = f\"ensemble-xgboost-{dataset_version}-{timestamp}\"\n",
    "\n",
    "    train_s3 = session.upload_data(train_file, bucket=bucket, key_prefix=f\"{prefix}/train\")\n",
    "    val_s3 = session.upload_data(val_file, bucket=bucket, key_prefix=f\"{prefix}/validation\")\n",
    "    test_s3 = session.upload_data(test_file, bucket=bucket, key_prefix=f\"{prefix}/test\")\n",
    "\n",
    "    print(\"Data uploaded to S3\")\n",
    "\n",
    "    # 5. Get XGBoost container\n",
    "    container = image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "    # 6. Create XGBoost estimator with ensemble-friendly hyperparameters\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",  # Larger instance for ensemble\n",
    "        output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "        sagemaker_session=session,\n",
    "    )\n",
    "\n",
    "    # Set ensemble-optimized hyperparameters\n",
    "    estimator.set_hyperparameters(\n",
    "        objective=\"binary:logistic\",\n",
    "        num_round=100,  # More rounds for better convergence\n",
    "        eta=0.1,  # Lower learning rate for stability\n",
    "        max_depth=8,  # Deeper trees for complex patterns\n",
    "        subsample=0.8,  # Subsampling for ensemble effect\n",
    "        colsample_bytree=0.8,  # Column sampling for ensemble effect\n",
    "        min_child_weight=3,  # Regularization\n",
    "        gamma=0.1,  # Regularization\n",
    "        eval_metric=\"auc\"\n",
    "    )\n",
    "\n",
    "    # 7. Train model with validation set\n",
    "    print(\"Training ensemble model...\")\n",
    "    train_input = sagemaker.inputs.TrainingInput(train_s3, content_type='text/csv')\n",
    "    val_input = sagemaker.inputs.TrainingInput(val_s3, content_type='text/csv')\n",
    "\n",
    "    estimator.fit({'train': train_input, 'validation': val_input})\n",
    "    print(\"Ensemble training completed!\")\n",
    "\n",
    "    # 8. Host the model on another instance\n",
    "    print(\"Hosting model on ml.m5.xlarge instance...\")\n",
    "\n",
    "    # Create transformer for batch inference\n",
    "    transformer = estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',  # Different instance for hosting\n",
    "        output_path=f's3://{bucket}/{prefix}/batch-transform',\n",
    "        strategy='MultiRecord',\n",
    "        assemble_with='Line',\n",
    "        accept='text/csv'\n",
    "    )\n",
    "\n",
    "    # 9. Perform batch transform to evaluate on testing data\n",
    "    print(\"Running batch transform on test data...\")\n",
    "    transformer.transform(\n",
    "        data=test_s3,\n",
    "        content_type='text/csv',\n",
    "        split_type='Line'\n",
    "    )\n",
    "\n",
    "    transformer.wait()\n",
    "    print(\"Batch transform completed!\")\n",
    "\n",
    "    # 10. Get predictions and calculate metrics\n",
    "    s3_client = boto3.client('s3')\n",
    "    predictions_key = f\"{prefix}/batch-transform/test_ensemble_{dataset_version}.csv.out\"\n",
    "\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=predictions_key)\n",
    "        predictions_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        # Parse predictions\n",
    "        pred_probs = []\n",
    "        for line in predictions_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    pred_probs.append(float(line.strip()))\n",
    "                except:\n",
    "                    pred_probs.append(0.5)\n",
    "\n",
    "        print(f\"Parsed {len(pred_probs)} ensemble predictions\")\n",
    "\n",
    "        # Convert probabilities to labels\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in pred_probs]\n",
    "\n",
    "        # 11. Report comprehensive performance metrics\n",
    "        metrics = calculate_comprehensive_metrics(y_test.values, pred_labels, pred_probs)\n",
    "\n",
    "        print(f\"\\nENSEMBLE MODEL PERFORMANCE METRICS ({dataset_version.upper()}):\")\n",
    "        print(f\"   Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "        print(f\"   Confusion Matrix:\")\n",
    "        print(f\"     - True Positives:  {metrics['confusion_matrix']['true_positive']}\")\n",
    "        print(f\"     - False Positives: {metrics['confusion_matrix']['false_positive']}\")\n",
    "        print(f\"     - True Negatives:  {metrics['confusion_matrix']['true_negative']}\")\n",
    "        print(f\"     - False Negatives: {metrics['confusion_matrix']['false_negative']}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting ensemble predictions: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "print(\"STARTING COMPREHENSIVE MODEL COMPARISON\")\n",
    "\n",
    "datasets = [\n",
    "    (\"combined_cvs_v1_small.csv\", \"v1\"),\n",
    "    (\"combined_cvs_v2_small.csv\", \"v2\")\n",
    "]\n",
    "\n",
    "results = {}\n",
    "ensemble_results = {}\n",
    "\n",
    "# Run simple models comparison\n",
    "for dataset, version in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {version.upper()}: {dataset}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Run simple models\n",
    "    try:\n",
    "        linear_acc = simple_train_test(dataset, \"linear\")\n",
    "    except Exception as e:\n",
    "        print(f\"Linear Learner failed: {e}\")\n",
    "        linear_acc = 0.0\n",
    "\n",
    "    try:\n",
    "        xgb_acc = simple_train_test(dataset, \"xgboost\")\n",
    "    except Exception as e:\n",
    "        print(f\"XGBoost failed: {e}\")\n",
    "        xgb_acc = 0.0\n",
    "\n",
    "    results[version] = {\n",
    "        'linear': linear_acc,\n",
    "        'xgboost': xgb_acc\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{version.upper()} SIMPLE MODELS RESULTS:\")\n",
    "    print(f\"  Linear Learner: {linear_acc:.4f}\")\n",
    "    print(f\"  XGBoost: {xgb_acc:.4f}\")\n",
    "\n",
    "# Step 3: Build and evaluate ensemble models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STEP 3: BUILDING AND EVALUATING ENSEMBLE MODELS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for dataset, version in datasets:\n",
    "    print(f\"\\nBUILDING ENSEMBLE MODEL FOR {version.upper()}\")\n",
    "    ensemble_metrics = build_ensemble_model(dataset, version)\n",
    "    ensemble_results[version] = ensemble_metrics\n",
    "\n",
    "# FINAL COMPARISON\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL COMPREHENSIVE COMPARISON: SIMPLE vs ENSEMBLE MODELS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for version in datasets:\n",
    "    version_key = version[1]\n",
    "    simple_linear = results[version_key]['linear']\n",
    "    simple_xgb = results[version_key]['xgboost']\n",
    "    ensemble_metrics = ensemble_results[version_key]\n",
    "\n",
    "    print(f\"\\nDATASET {version_key.upper()} COMPARISON:\")\n",
    "    print(f\"\\n   SIMPLE MODELS:\")\n",
    "    print(f\"     Linear Learner: {simple_linear:.4f}\")\n",
    "    print(f\"     XGBoost:       {simple_xgb:.4f}\")\n",
    "\n",
    "    if ensemble_metrics:\n",
    "        print(f\"\\n   ENSEMBLE MODEL (XGBoost Optimized):\")\n",
    "        print(f\"     Accuracy:      {ensemble_metrics['accuracy']:.4f}\")\n",
    "        print(f\"     Precision:     {ensemble_metrics['precision']:.4f}\")\n",
    "        print(f\"     Recall:        {ensemble_metrics['recall']:.4f}\")\n",
    "        print(f\"     F1-Score:      {ensemble_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        # Performance comparison\n",
    "        best_simple = max(simple_linear, simple_xgb)\n",
    "        ensemble_acc = ensemble_metrics['accuracy']\n",
    "        improvement = ensemble_acc - best_simple\n",
    "\n",
    "        print(f\"\\n PERFORMANCE COMPARISON:\")\n",
    "        print(f\"     Best Simple Model:    {best_simple:.4f}\")\n",
    "        print(f\"     Ensemble Model:       {ensemble_acc:.4f}\")\n",
    "        print(f\"     Improvement:          {improvement:+.4f}\")\n",
    "\n",
    "        if improvement > 0:\n",
    "            print(f\"Ensemble model performed better by {improvement:.4f}\")\n",
    "        elif improvement < 0:\n",
    "            print(f\"Simple model performed better by {-improvement:.4f}\")\n",
    "        else:\n",
    "            print(f\"Both approaches performed equally\")\n",
    "\n",
    "        print(f\"\\nOBSERVATIONS:\")\n",
    "        if improvement > 0.02:\n",
    "            print(f\"     - Ensemble model significantly outperforms simple models\")\n",
    "            print(f\"     - The ensemble approach better captures complex patterns\")\n",
    "            print(f\"     - Additional training rounds and regularization helped\")\n",
    "        elif improvement > 0:\n",
    "            print(f\"     - Ensemble model shows modest improvement\")\n",
    "            print(f\"     - The dataset might have simpler patterns\")\n",
    "            print(f\"     - Ensemble benefits are present but limited\")\n",
    "        else:\n",
    "            print(f\"     - Simple models are sufficient for this dataset\")\n",
    "            print(f\"     - Ensemble complexity doesn't provide additional benefits\")\n",
    "            print(f\"     - Consider feature engineering for better performance\")\n",
    "    else:\n",
    "        print(f\"\\n Ensemble model failed for {version_key.upper()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE ANALYSIS COMPLETED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d4b58-41be-4901-9a53-c649f86287a7",
   "metadata": {
    "id": "df1d4b58-41be-4901-9a53-c649f86287a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
